%% jdavid-jdecker-shrooms.tex
%% Jon David and Jarrett Decker
%% Sunday, February 14, 2016
%%
%%-------------------------------------------------------------------
%% Notes to self:
%%
%% Package algpseudocode and algorithm need to be installed. Try:
%%   sudo apt-get install texlive-science

\documentclass{IEEEtran}

\usepackage[style=ieee,backend=bibtex]{biblatex}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
%%\usepackage{appendix}


\hypersetup{hidelinks}

\addbibresource{quinlan1986.bib}
\addbibresource{schlimmer1981.bib}
\addbibresource{mitchell1997.bib}


\author{Jon David*\thanks{e-mail:
    \href{mailto:jdavid@cs.unm.edu}
         {\texttt{jdavid@cs.unm.edu}}
         {*authors contributed equally}} and
\and
       Jarrett Decker*\thanks{e-mail:
    \href{mailto:p.s.ortegel@gmail.com}
         {\texttt{jdeck069@unm.edu}}
         {*authors contributed equally}}}

\title{An Implementation of a Decision Tree to Classify Poisonous and
  Edible Mushrooms}

\begin{document}

\maketitle

\begin{abstract}

The problem we have been asked to solve is to classify mushrooms as
either edible or poisonous. Our data contains 22 different non-numeric
attributes to help us in this endeavor. This is a perfect candidate
for the use of a Decision Tree and will allow us to hopefully come up
with a few rules to help us identify poisonous mushrooms in the real
world. 
\end{abstract}

\section{Background on Decision Trees}
We should cite \parencite{mitchell1997machine} here.
Typically, a machine learning algorithm works by taking a known set of
data and devising a function that generalizes it. Computers allow us
to define multidimensional data sets and find these functions almost
effortlessly. One of the drawbacks of these approaches, however, is
that these functions are usually incomprehensible to humans. Most
machine learning algorithms also only deal with numerical
data. Decision Trees are a method which overcomes both of these
shortcomings. Decision Trees create a system of rules that a human
could read and understand, and when the rules are applied to new data,
allows for fast classification.

%Here is \parencite{wada2012incremental} just for kicks.

\section{Design and Implementation}
As per the project instructions our Decision Tree is modeled as the
ID3 algorithm. In an effort to maintain good software engineering
practices, we also chose to use object oriented programming
methodologies when coding our Decision Tree. Our first challenge was
getting the data in a structure that we could manage and
manipulate. We then started defining our main classes for the tree. We
implemented both entropy and misclassification error for our
information gain metrics. After we had information gain we could then
construct the tree. Pruning via chi square calculations was the next
mechanic needed. We then were able to create our classification
functionality.

\subsection{Data Structures}
Data sets lie at the heart of all machine learning methods so being
able to work with the data is very important. The approach we took was
to create two classes, a database class, and a datadef class. The
database class would essentially be a list of records created from the
raw data set we were given. A record is a set of dictionaries where
the dictionary key is a string representing an attribute and the value
is the value of that attribute, for every attribute of a single row in
our data set. The datadef class reads in a definition file that lists
every attribute and every possible valid symbol for that
attribute. This was useful for iterating through all possible symbols
of an attribute. This approach would also allow us to possibly define
new types of data sets at runtime, instead of having to hard code
attribute definitions. 

We also wrote a few helper functions to grab a single column of a
database. For instance, grabbing a vector of just the labels of each
record in a database was an easy way to put together an output file
after classification of that database.

\subsection{Tree Building}
Our tree is made up of a few classes. The ID3Tree class is the highest
level class which is composed of ID3Nodes and ID3Edges. The ID3Node
class is inherited from by the ID3DecisionNode and ID3LeafNode
classes. Leaf nodes only keep track of the label they represent while
decision nodes know what attribute they split on, how much information
gain they represent, and their chi squared value. Edges have a source
node and a destination node, the attribute that caused the split, and
what value specifically this edge represents. This infrastructure
allowed us to see exactly how the tree was architected and comprehend
what was going on at any point in the tree. The ID3Tree class
implemented most of the functions needed to construct the tree as well
as interact with it through classification.

\subsection{ID3 Algorithm}
With our infrastructure, information gain functions, and our chi
square check, we could now create a decision tree with the ID3
algorithm. The algorithm works by taking in a database object, an
information gain object, attribute definitions, and a chi square table
with a confidence interval. A node is created for the database and
then the database is checked for class impurities. If the database
only has a single label then it is deemed pure and the node is
returned as a leaf node. If the node isn't pure then the database
is analyzed by the information gain class to find a candidate
attribute to split on.

Stuff with chi square happens too.

When an attribute is found the ID3 algorithm recursively calls itself to
create nodes out of the split portions of the database and finishes
either when each node hits a pure set or when the chi square test
halts splitting. This function returns a tree object which can then be 
used for classification.

Our implementation is based on the ID3 algorithm described
in \parencite{quinlan1986induction} and is defined in Algorithm 1.


\begin{algorithm}
\label{id3algorithm}
\caption{id3($C$, $D$, $T$, $A$, $\alpha$)}
\begin{algorithmic}
\If{$D$'s class is homogeneous}
  \State $label$ $\leftarrow$ mode($D$['class'])
  \State return ID3Tree( ID3LeafNode($label$) )
\EndIf
\If{$A$ = $\emptyset$}
  \State $label$ $\leftarrow$ mode($D$[T])
  \State return ID3Tree( ID3LeafNode($label$) )
\EndIf

\State Let $R$ be $C$'s recommended attribute
\State $decisionnode$ $\leftarrow$ ID3DecisionNode($R$)

\State $tree$ $\leftarrow$ ID3Tree($decisionnode$)
\For{$v$ $\in$ val($R$)}
  \State $\chi^2$ $\leftarrow$ calculate $\chi^2$ using $R$, $D$
  \State $dof$ $\leftarrow$ (number of valid values in $A$) - 1
  \If{ShouldPrune($\chi^2$, $dof$, $\alpha$)}
    \State $label$ $\leftarrow$ mode($D$[$T$])
    \State return ID3Tree( ID3Leaf($label$) )
  \EndIf

  \State $edge$ $\leftarrow$ new ID3Edge($R$, $v$)
  \State Let $D_v$ $\subseteq$ $D$ such that $D$[$R$] = $v$
  \If{$D_v$ = $\emptyset$}
    \State $label$ $\leftarrow$ mode($D$['class'])
    \State $leafnode$ $\leftarrow$ ID3LeafNode[$label$]
    \State $tree$.add($decisionnode$, $edge$, $leafnode$)
  \Else
    \State $subtree$ $\leftarrow$ id3($C$, $D_v$, $T$, $A$-$R$, $\alpha$)
    \State $tree$.addtree($decisionnode$, $edge$, $subtree$)
  \EndIf

  return $tree$
\EndFor

\end{algorithmic}
\end{algorithm}


\subsection{Attribute Selection}
The two methods of information gain we needed to use were entropy and
misclassification error. We created two classes to deal with this,
InformationGainCriteria and ClassificationErrorCriteria. The purpose
of these classes were to be able to read through a database and then
produce the best attribute candidate for splitting the database at the
given node. Both of these methods work in similar ways. Entropy is
calculated as -P(x)log(P(x)) and InformationGainCriteria would take
the entropy at the parent node and subtract the weighted entropy of
each of the child nodes after a simulated split for each possible
attribute.

\subsubsection{Selection via information gain}
Algorithm for selecting the best attribute using information gain.

We use the definition of entropy as defined in \parencite{mitchell1997machine}:
\begin{equation}
\label{entropy-equation}
Entropy(S) = \sum_{v\in val(A)}-p_ilog_2(p_i)
\end{equation}

Equation for information gain:
\begin{equation}
\label{information-gain-equation}
Gain(S,A) = Entropy(S) -\sum_{v \in val(A)}\frac{|S_v|}{|S|}Entropy(S_v)
\end{equation}


We select the attribute that offers the greatest information gain.

\subsubsection{Selection via misclassification error}
Algorithm for selecting the best attribute using misclassification
error.

Equation for misclassification error.
\begin{equation}
\label{misclassification-error-equation}
M_E = 1 - max(p_1, p_2, ..., p_n) 
\end{equation}
We select the attribute that offers the minimal misclassification
error.


\subsection{Overfitting and Branch Pruning}
A tendency for decision tree models is to overfit the training
examples \parencite{mitchell1997machine}. Overfitting means that the
model performs well when classifying examples it has seen before, but
performs poorly when attempting to classify new, unseen, examples.

One method to overcome overfitting is through
pruning. \parencite{quinlan1986induction} proposes the use of the
chi-square test for stochastic independence to test whether or not
there is indeed an advantage gained by splitting.

\begin{equation}
\label{chi-square-equation}
\chi^2 = \sum_{v\in val(A)}\frac{(p_i-p'_i)^2}{p'_i} + \
\frac{(e_i-e'_i)^2}{e'_i}
\end{equation}
Where,
\begin{equation}
\label{chi-square-helper-equation}
p'_i = p \times \frac{p_i+e_i}{p+e}
\end{equation}

\subsection{Classification}
Stuff about classification. Now here's the classify algorithm.
\begin{algorithm}
\caption{tree.classify(node, record)}
\begin{algorithmic}
\If{$node$ is $None$}
  \State return most common class in this tree
\EndIf
\If{$node$ is a ID3Leaf}
  \State return $node.classification$
\EndIf
\State $nextnode$ $\leftarrow$ $None$
\For {each $edge$ emerging from $node$}
  \State $attribute$ $\leftarrow$ $edge.branchattribute$
  \If {$edge.branchvalue$ = $record$[$attribute$]}
    \State $nextnode$ $\leftarrow$ $edge.destinationnode$
    \State break
  \EndIf
\EndFor
\State return classify($nextnode$, $record$)
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\subsection{Data}
The data used to train the decision tree model can be found in the
University of California, Irvine's Machine Learning
Repository \parencite{schlimmer1981mushroom}. It is a multivariate dataset
consisting of 22 categorical attributes that describe mushroom
characteristics and a label. In this dataset a mushroom is labeled to
be poisonous or edible. This type of dataset is appropriate for
classification tasks.

The dataset in the repository contains 8124 instances. These instances
are partitioned into three files: (1) a training set with 4062 instances
(50\%), (2) a testing set with 2031 instances (25\%), and (3) a
validation set with 2031 instances (25\%).

\subsection{Methods}
Decision trees are built using the ID3 algorithm described in the
implementation section. Different decision trees are built by
specifying two parameters: (1) attribute-selection criteria, which
determines which attribute will be used to further split the tree, and (2)
confidence-interval, which specifies how much certainty is required before
continuing to split the decision tree.  There are two
attribute-selection criteria: information-gain and
misclassification-error; and four confidence intervals of interest:
99\%,  95\%, 50\%, and 0\%. The 0\% confidence interval means the tree
will always be fully grown, never pruned.

From the two attribute-selection criteria and four confidence
intervals eight decision trees are built and evaluated.

\subsection{Results}
All models performed equally well over the test and validation
sets. In addition,  it was a surprise that no trees were pruned. The
table below shows the confusion matrix shared by all 8 models.

\begin{table}[ht]
  \caption{Confusion Matrix}
  \centering
  \begin{tabular}{c c c }
  \hline\hline
                & Predicted num(p) & Predicted num(e) \\ [0.5ex]
  %heading
  \hline
  Actual num(p) &              985 &                0 \\
  Actual num(e) &                0 &             1048 \\ [1ex]
  \hline
  \end{tabular}
  \label{table:nonlin}
\end{table}

A confusion matrix is more descriptive of a model's performance than
accuracy. A confusion matrix shows the number of true positives (TP), true
negatives (TN), false positives (FP), and false negatives (FN). From this accuracy
and misclassification can be calculated. Accuracy is (TP+TN/p+e) and
Misclassification is 1.0-Accuracy.

In contrast, when evaluating the model over a validation set the label
is not given. The only information provided is accuracy. The table
below shows the classification accuracy of each model over the
validation set.

\begin{table}[ht]
  \caption{Classification Accuracy}
  \centering
  \begin{tabular}{c c c c c}
  \hline\hline
  Criteria & $\alpha$=0.01 & $\alpha=0.05$ & $\alpha=0.50$ & $\alpha$=0.0 \\ [0.5ex]
  %heading
  \hline
  information-gain & 99.95\% & 99.95\% & 99.95\% & 99.95\% \\
  misclass-error   & 99.95\% & 99.95\% & 99.95\% & 99.95\% \\ [1ex]
  \hline
  \end{tabular}
  \label{table:nonlin}
\end{table}

Although the trees built using the information-gain criteria performed
equally as well as the trees built using the misclassification-error
criteria, the two trees are slightly different. The trees built using
the information-gain criteria contained only 38 nodes with a max depth
of 4, whereas the trees built using the misclassification-error
criteria contained 45 nodes with a max depth of 5. Even though the two
types of trees performed equally well, the simpler tree with fewer
nodes and smaller depth is the preferred model.

\section{Discussion}
\subsection{Explanation of Results}
All eight trees produced from information-gain criteria,
misclassification-error criteria, $\alpha$=0.01, $\alpha$=0.05, $\alpha$=0.50,
$\alpha$=0.0 perform equally well. It is surprising but this may be because
the dataset is well suited for decision tree learning. Decision tree
learning is best suited to problems where instances are represented as
attribute-value pairs, and the target function has discrete output
values \parencite{mitchell1997machine}. These are characteristics
found in the mushroom database.

\subsection{Proposed Classification Rules}
The Appendix contains the complete set of classification rules
constructed from one of our decision trees (one built using the
information-gain critiera and $\alpha$=0.01). 

A subset of those rules that identify poisonous mushrooms is listed
here: (1) if the mushroom smells creosote, musty, foul, pungent,
fishy, or spicy, then it is poisonous; (2) if the mushroom has no
scent and its spore-print-color is green, then it is poisonous; (3) if
the mushroom has no odor, has a white spore-print-color, and its
stalk-root has a club shape, then it is poisonous; (4) if the mushroom
has no odor, has a white spore-print-color, and its gill-size is
narrow, then it is poisonous; and finally (5) if the mushroom has no
odor, has a white spore-print-color, a bulbous stalk-root, and a white
cap, then it is poisonous. 


\section{Conclusions}

\printbibliography

\newpage
\appendix
\section{ Classification Rules}
These are the classification rules produced by a decision tree using
information-gain criteria and $\alpha$=0.01. These rules are equivalent to
the rules produced for other information-gain trees with $\alpha$=0.05,
$\alpha$=0.50, and $\alpha$=0.0.

\texttt{[R1] IF ((odor=c)), THEN p.}

\texttt{[R2] IF ((odor=m)), THEN p.}

\texttt{[R3] IF ((odor=l)), THEN e.}

\texttt{[R4] IF ((odor=f)), THEN p.}

\texttt{[R5] IF ((odor=p)), THEN p.}

\texttt{[R6] IF ((odor=a)), THEN e.}

\texttt{[R7] IF ((odor=y)), THEN p.}

\texttt{[R8] IF ((odor=s)), THEN p.}

\texttt{[R9] IF ((odor=n) AND (spore-print-color=h)), THEN e.}

\texttt{[R10] IF ((odor=n) AND (spore-print-color=o)), THEN e.}

\texttt{[R11] IF ((odor=n) AND (spore-print-color=r)), THEN p.}

\texttt{[R12] IF ((odor=n) AND (spore-print-color=b)), THEN e.}

\texttt{[R13] IF ((odor=n) AND (spore-print-color=w) AND
  (stalk-root=c)), THEN p.}

\texttt{[R14] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=?)
  AND (gill-size=b)), THEN e.}

\texttt{[R15] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=?)
  AND (gill-size=n)), THEN p.}

\texttt{[R16] IF ((odor=n) AND (spore-print-color=w) AND
  (stalk-root=r)), THEN e.}

\texttt{[R17] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=z)), THEN
e.}

\texttt{[R18] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=c)), THEN e.}

\texttt{[R19] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=r)), THEN e.}

\texttt{[R20] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=b)), THEN e.}

\texttt{[R21] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=p)), THEN e.}

\texttt{[R22] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=w)), THEN p.}

\texttt{[R23] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=y)), THEN e.}

\texttt{[R24] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=n)), THEN e.}

\texttt{[R25] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=e)), THEN e.}

\texttt{[R26] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=u)), THEN e.}

\texttt{[R27] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=g)), THEN e.}

\texttt{[R28] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=u)), THEN
e.}

\texttt{[R29] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=e)), THEN
e.}

\texttt{[R30] IF ((odor=n) AND (spore-print-color=y)), THEN e.}

\texttt{[R31] IF ((odor=n) AND (spore-print-color=n)), THEN e.}

\texttt{[R32] IF ((odor=n) AND (spore-print-color=u)), THEN e.}

\texttt{[R33] IF ((odor=n) AND (spore-print-color=k)), THEN e.}



\end{document}
