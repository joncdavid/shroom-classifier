%% jdavid-jdecker-shrooms.tex
%% Jon David and Jarrett Decker
%% Sunday, February 14, 2016
%%
%%-------------------------------------------------------------------
%% Notes to self:
%%
%% Package algpseudocode and algorithm need to be installed. Try:
%%   sudo apt-get install texlive-science

\documentclass{IEEEtran}

\usepackage[style=ieee,backend=bibtex]{biblatex}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
%%\usepackage{appendix}


\hypersetup{hidelinks}

\addbibresource{quinlan1986.bib}
\addbibresource{schlimmer1981.bib}
\addbibresource{mitchell1997.bib}


\author{Jon David*\thanks{e-mail:
    \href{mailto:jdavid@cs.unm.edu}
         {\texttt{jdavid@cs.unm.edu}}
         {*authors contributed equally}} and
\and
       Jarrett Decker*\thanks{e-mail:
    \href{mailto:p.s.ortegel@gmail.com}
         {\texttt{jdeck069@unm.edu}}
         {*authors contributed equally}}}

\title{An Implementation of a Decision Tree to Classify Poisonous and
  Edible Mushrooms}

\begin{document}

\maketitle

\begin{abstract}

Mushrooms are delectable ingredients that are difficult for amateurs
to forage and cultivate. A common problem amateur mushroom foragers
face is identifying edible and poisonous mushrooms. Incorrectly
classifying a mushroom as edible could lead to mushroom poisoning,
with symptoms ranging between gastrointestinal discomfort to death. In
this paper we discuss a machine learning approach to classify these
mushrooms. We implement and train decision trees and evaluate their
performance over several parameters such as attribute-selection
criteria, and confidence level. The experiments show that after
training, all of the generated decisions trees perform very well. They
all produce 99\% accuracy on the validation set.

\end{abstract}

\section{Introduction}
Machine learning is an approach to problem solving that involves
training a model on a large set of examples to extract generalized
knowledge and to make predictions on new examples. It requires a
well-specified task, a measure of performance, and a training
examples \parencite{mitchell1997machine}.

The classification of poisonous and edible mushrooms is a
well-specified task. It's measure of performance is the accuracy of
its predictions. And a large dataset of 8124 total mushroom instances is
available, 4062 of which is allocated as training examples. This means that
a machine learning approach is a viable option to solving the task of
classifying poisonous and edible mushrooms. Decision trees in
particular are suitable for this task \parencite{mitchell1997machine}
because  data is represented as attribute-value pairs and, the target
function's range is a set of categorical values.

One of the main advantages of using decision trees is their reasoning
mechanism is easy to understand. When a decision tree classifies an
instance its reasoning mechanism can be traced back from the leaf node
to the root.

Another advantage decision trees have is their structure. Their tree
structure allows for classification in $\mathcal{O}(log_{v}n)$ time,
where $v$ is the maximum number of valid values for any given attribute.


\section{Design and Implementation}
As per the project instructions our Decision Tree is modeled as the
ID3 algorithm. In an effort to maintain good software engineering
practices, we also chose to use object oriented programming
methodologies when coding our Decision Tree. Our first challenge was
getting the data in a structure that we could manage and
manipulate. We then started defining our main classes for the tree. We
implemented both entropy and misclassification error for our
information gain metrics. After we had information gain we could then
construct the tree. Pruning via chi square calculations was the next
mechanic needed. We then were able to create our classification
functionality.

\subsection{Data Structures}
The ShroomDefs class abstracts the definition of a mushroom
instance. It defines the set of valid class values, the set of valid
attributes, and the set of valid symbols for each attribute. This
approach would also allow us to possibly define new types of data sets
at runtime, or to define a new set of attribute and valid values
simply by defining a new file. 

The ShroomRecord and ShroomDatabase classes abstract the dataset and
provide functions to access data without having to understand its
underlying implementation. For example, the database class provides
functions to access an attribute columns which make it
easier to calculate entropy because the task of extracting those
values has already been taken care of.

The ID3Tree is composed of ID3Nodes and ID3Edges. The connectivity
between nodes and edges is defined by an adjacency matrix. The
ID3Nodes have subclasses ID3DecisionNode and ID3LeafNode. ID3LeafNodes
are a simple class that keep track of only their classification
class. ID3DecisionNodes keep track of which attribute they split on,
how much information they gain, and their chi-squared value. ID3Edges
are composed of a source and destination node, and their branch
attribute and branch value.

The ID3Tree class, and the ID3Nodes and ID3Edges classes it contains,
abstract the implementation of adding nodes and trees to existing
trees, making it easier for the ID3 algorithm to build a tree. The
ID3Tree also abstracts the mechanism of classifying instances.


\subsection{ID3 Algorithm}
The algorithm generates an ID3Tree. It first checks for homogeneity
among the dataset's class values. If the dataset's class values are
pure, (i.e., homogeneous), ID3 returns a new subtree with a single
ID3LeafNode that classifies that dataset.

If the dataset is not homogeneous but the set of attributes is empty,
that means there are no more attributes to split off of. ID3 returns a
new subtree with a single ID3LeafNode that classifies the majority of
the classes in the dataset.

Otherwise, ID3 uses a attribute-selection criteria (either
information-gain or misclassification-error) to determine the next
attribute to split on. When an attribute is found the ID3 algorithm
recursively calls itself to create nodes out of the split portions of
the database and finishes either when each node hits a pure set or
when the $\chi^2$ test halts splitting. This function returns a tree
object which can then be used for classification.

Our implementation is based on the ID3 algorithm described
in \parencite{quinlan1986induction} and with our own version defined
in Algorithm 1. 


\begin{algorithm}
\label{id3algorithm}
\caption{id3($C$, $S$, $T$, $A$, $\alpha$)}
\begin{algorithmic}
\Statex \Comment{ \% Input: attribute-selection criteria $C$,  \%}
\Statex \Comment{ \% Input: set of examples $S$, target attribute $T$ \%}
\Statex \Comment{ \% Input: set of attributes $A$, 1-confidence $\alpha$  \%}
\If{$S$'s class is homogeneous}
  \State $label$ $\leftarrow$ mode($S$['class'])
  \State return ID3Tree( ID3LeafNode($label$) )
\EndIf
\If{$A$ = $\emptyset$}
  \State $label$ $\leftarrow$ mode($S$[T])
  \State return ID3Tree( ID3LeafNode($label$) )
\EndIf

\State Let $R$ be $C$'s recommended attribute
\State $decisionnode$ $\leftarrow$ ID3DecisionNode($R$)

\State $tree$ $\leftarrow$ ID3Tree($decisionnode$)
\For{$v$ $\in$ val($R$)}
  \State $\chi^2$ $\leftarrow$ calculate $\chi^2$ using $R$, $S$
  \State $dof$ $\leftarrow$ (number of valid values in $A$) - 1
  \If{not ShouldPrune($\chi^2$, $dof$, $\alpha$)}
    \State $label$ $\leftarrow$ mode($S$[$T$])
    \State return ID3Tree( ID3Leaf($label$) )
  \EndIf

  \State $edge$ $\leftarrow$ new ID3Edge($R$, $v$)
  \State Let $S_v$ $\subseteq$ $S$ such that $S$[$R$] = $v$
  \If{$S_v$ = $\emptyset$}
    \State $label$ $\leftarrow$ mode($S$['class'])
    \State $leafnode$ $\leftarrow$ ID3LeafNode[$label$]
    \State $tree$.add($decisionnode$, $edge$, $leafnode$)
  \Else
    \State $subtree$ $\leftarrow$ id3($C$, $S_v$, $T$, $A$-$\lbrace$$R$$\rbrace$, $\alpha$)
    \State $tree$.addtree($decisionnode$, $edge$, $subtree$)
  \EndIf

  return $tree$
\EndFor

\end{algorithmic}
\end{algorithm}


\subsection{Attribute Selection}
Attribute selection is the process of selecting the attribute which
best partitions the dataset such that the subset datasets become more
homogeneous and easier to classify.

Two methods implement attribute selection. The first method seeks to
maximize information gain and the second seeks to minimize
misclassification error.

\subsubsection{Attribute selection via information gain}
Attribute selection via information gain makes use of an idea borrowed
from information theory.

We use the definition of entropy as defined
in \parencite{mitchell1997machine}:
\begin{equation}
\label{entropy-equation}
Entropy(S) = \sum_{v\in val(A)}-p_ilog_2(p_i)
\end{equation}

Where $S$ is the dataset at the current node, $A$ is the current
attribute of interest, and $p_i$ is the proportion of samples $\in$ $S$
whose attribute value equals $v$.

The entropy function is used when calculating the information gain on
each attribute. We use the definition of information gain as defined
in \parencite{mitchell1997machine}:
\begin{equation}
\label{information-gain-equation}
Gain(S,A) = Entropy(S) -\sum_{v \in val(A)}\frac{|S_v|}{|S|}Entropy(S_v)
\end{equation}

The best attribute is the one with the greatest information gain. The
information gain algorithm is defined in algorithm 2.

\begin{algorithm}
\caption{calculate information gain($A$, $S$)}
\begin{algorithmic}
\Statex\Comment{ \%Input: attribute A, dataset S\%}
\State $entropybefore$ $\leftarrow$ Entropy($S$)
\State $entropyafter$ $\leftarrow$ 0.0
\For{$v$ $\in$ vals($A$)}
  \State Let $S_v$ $\subseteq$ $S$, such that $S$[$A$] = $v$
  \State $localentropy$ $\leftarrow$ $\frac{|S_v|}{|S|}$ $\times$ Entropy($S_v$)
  \State $entropyafter$ $\leftarrow$ $entropyafter$ + $localentropy$
\EndFor

\State $gain$ $\leftarrow$ $entropybefore$ - $entropyafter$
\State return $gain$
\end{algorithmic}
\end{algorithm}


\subsubsection{Attribute selection via misclassification error}
Attribute selection via misclassification error criteria is a more
na{\"i}ve method. But it's easier to calculate. We define the
misclassification error as follows:

\begin{equation}
\label{misclassification-error-equation}
M_E(S,A) = 1 - max(p_a, p_b, ..., p_k) 
\end{equation}

Where, $S$ is the dataset available at the current node, $A$ is the
attribute we are calculating the misclassification error for, and $p_x$
is the proportion of samples in $S$ such that $S$[$A$] = $x$.

After calculating the misclassification error for each attribute, the
best attribute is the one that offers the minimal misclassification
error.

\begin{algorithm}
\caption{calculate misclassification error($A$, $S$)}
\begin{algorithmic}
\Statex \Comment{\%Input: attribute $A$, dataset $S$ \%}
\State Let $T$ be a frequency table
\State Let $P$ be a table of proportions
\For{each $v$ $\in$ val($A$)}
  \For{each $example$ $\in$ $S$}
    \State $T$[$v$] $\leftarrow$ $T$[$v$] + 1, if $example$[$A$]=$v$
  \EndFor
  \State $P$[$v$] $\leftarrow$ $T$[$v$] / $\lvert S \rvert$
\EndFor
\State return 1-max($\lbrace p \mid p \in P\rbrace$)

\end{algorithmic}
\end{algorithm}

\subsection{Overfitting and Branch Pruning}
A tendency for decision tree models is to overfit the training
examples \parencite{mitchell1997machine}. Overfitting means that the
model performs well when classifying examples it has seen before, but
performs poorly when attempting to classify new, unseen, examples.

One method to overcome overfitting is through
pruning. \parencite{quinlan1986induction} proposes the use of the
chi-square test for stochastic independence to test whether or not
there is indeed an advantage gained by splitting.

\begin{equation}
\label{chi-square-equation}
\chi^2 = \sum_{v\in val(A)}\frac{(p_i-p'_i)^2}{p'_i} + \
\frac{(e_i-e'_i)^2}{e'_i}
\end{equation}
Where,
\begin{equation}
\label{chi-square-helper-equation}
p'_i = p \times \frac{p_i+e_i}{p+e}
\end{equation}

When the calculated $\chi^2$ value is greater than the value provided by
the $\chi^2$ distribution table, continue splitting the tree. Otherwise,
stop splitting. This method is called split-stopping and is one method
of preventing a decision tree from overfitting.


\subsection{Classification}
Classification is the process of using a trained model to make
predictions on new examples. Specifically the decision tree predicts
whether a mushroom is poisonous or edible.

Because this model is a tree, it is simple to define a recursive
definition to classify an example. If the current node is a leaf,
return that leaf's classification; otherwise, it is a decision
node. Ask each of this decision node's children to classify themselves.


\begin{algorithm}
\caption{classify($node$, $example$)}
\begin{algorithmic}
\Statex \Comment{ \% Input: ID3Node $node$, $example$ \%}
\If{$node$ is $None$}
  \State return most common class in this tree
\EndIf
\If{$node$ is a ID3Leaf}
  \State return $node.classification$
\EndIf
\State $nextnode$ $\leftarrow$ $None$
\For {each $edge$ emerging from $node$}
  \State $attribute$ $\leftarrow$ $edge.branchattribute$
  \If {$edge.branchvalue$ = $example$[$attribute$]}
    \State $nextnode$ $\leftarrow$ $edge.destinationnode$
    \State break
  \EndIf
\EndFor
\State return classify($nextnode$, $example$)
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\subsection{Data}
The data used to train the decision tree model can be found in the
University of California, Irvine's Machine Learning
Repository \parencite{schlimmer1981mushroom}. It is a multivariate dataset
consisting of 22 categorical attributes that describe mushroom
characteristics and a label. In this dataset a mushroom is labeled to
be poisonous or edible. This type of dataset is appropriate for
classification tasks.

The dataset in the repository contains 8124 instances. These instances
are partitioned into three files: (1) a training set with 4062 instances
(50\%), (2) a testing set with 2031 instances (25\%), and (3) a
validation set with 2031 instances (25\%).

\subsection{Methods}
Decision trees are built using the ID3 algorithm described in the
implementation section. Different decision trees are built by
specifying two parameters: (1) attribute-selection criteria, which
determines which attribute will be used to further split the tree, and (2)
confidence-interval, which specifies how much certainty is required before
continuing to split the decision tree.  There are two
attribute-selection criteria: information-gain and
misclassification-error; and four confidence intervals of interest:
99\%,  95\%, 50\%, and 0\%. The 0\% confidence interval means the tree
will always be fully grown, never pruned.

From the two attribute-selection criteria and four confidence
intervals eight decision trees are built and evaluated.

\subsection{Results}
All models performed equally well over the test and validation
sets. In addition,  it was a surprise that no trees were pruned. The
table below shows the confusion matrix shared by all 8 models.

\begin{table}[ht]
  \caption{Confusion Matrix}
  \centering
  \begin{tabular}{c c c }
  \hline\hline
                & Predicted num(p) & Predicted num(e) \\ [0.5ex]
  %heading
  \hline
  Actual num(p) &              985 &                0 \\
  Actual num(e) &                0 &             1048 \\ [1ex]
  \hline
  \end{tabular}
  \label{table:nonlin}
\end{table}

A confusion matrix is more descriptive of a model's performance than
accuracy. A confusion matrix shows the number of true positives (TP), true
negatives (TN), false positives (FP), and false negatives (FN). From this accuracy
and misclassification can be calculated. Accuracy is (TP+TN/p+e) and
Misclassification is 1.0-Accuracy.

In contrast, when evaluating the model over a validation set the label
is not given. The only information provided is accuracy. The table
below shows the classification accuracy of each model over the
validation set.

\begin{table}[ht]
  \caption{Classification Accuracy}
  \centering
  \begin{tabular}{c c c c c}
  \hline\hline
  Criteria & $\alpha$=0.01 & $\alpha=0.05$ & $\alpha=0.50$ & $\alpha$=0.0 \\ [0.5ex]
  %heading
  \hline
  information-gain & 99.95\% & 99.95\% & 99.95\% & 99.95\% \\
  misclass-error   & 99.95\% & 99.95\% & 99.95\% & 99.95\% \\ [1ex]
  \hline
  \end{tabular}
  \label{table:nonlin}
\end{table}

Although the trees built using the information-gain criteria performed
equally as well as the trees built using the misclassification-error
criteria, the two trees are slightly different. The trees built using
the information-gain criteria contained only 38 nodes with a max depth
of 4, whereas the trees built using the misclassification-error
criteria contained 45 nodes with a max depth of 5. Even though the two
types of trees performed equally well, the simpler tree with fewer
nodes and smaller depth is the preferred model.

\section{Discussion}
\subsection{Explanation of Results}
All eight trees produced from information-gain criteria,
misclassification-error criteria, $\alpha$=0.01, $\alpha$=0.05, $\alpha$=0.50,
$\alpha$=0.0 perform equally well. It is surprising but this may be because
the dataset is well suited for decision tree learning. Decision tree
learning is best suited to problems where instances are represented as
attribute-value pairs, and the target function has discrete output
values \parencite{mitchell1997machine}. These are characteristics
found in the mushroom database.

\subsection{Proposed Classification Rules}
The Appendix contains the complete set of classification rules
constructed from one of our decision trees (one built using the
information-gain critiera and $\alpha$=0.01). 

A subset of those rules that identify poisonous mushrooms is listed
here: (1) if the mushroom smells creosote, musty, foul, pungent,
fishy, or spicy, then it is poisonous; (2) if the mushroom has no
scent and its spore-print-color is green, then it is poisonous; (3) if
the mushroom has no odor, has a white spore-print-color, and its
stalk-root has a club shape, then it is poisonous; (4) if the mushroom
has no odor, has a white spore-print-color, and its gill-size is
narrow, then it is poisonous; and finally (5) if the mushroom has no
odor, has a white spore-print-color, a bulbous stalk-root, and a white
cap, then it is poisonous. 


\section{Conclusions}
Given the performance of the decision trees it is tempting to suggest
that the classification of poisonous and edible mushrooms is an easy
task. However, the consumption of an incorrectly identified edible
mushroom can be fatal. It is important to remember that models are
representations of the things they are modeling and that no model is
ever 100\% complete. 

\printbibliography

\newpage
\onecolumn
\appendix
\section{ Classification Rules}
These are the classification rules produced by a decision tree using
information-gain criteria and $\alpha$=0.01. These rules are equivalent to
the rules produced for other information-gain trees with $\alpha$=0.05,
$\alpha$=0.50, and $\alpha$=0.0.

\texttt{[R1] IF ((odor=c)), THEN p.}

\texttt{[R2] IF ((odor=m)), THEN p.}

\texttt{[R3] IF ((odor=l)), THEN e.}

\texttt{[R4] IF ((odor=f)), THEN p.}

\texttt{[R5] IF ((odor=p)), THEN p.}

\texttt{[R6] IF ((odor=a)), THEN e.}

\texttt{[R7] IF ((odor=y)), THEN p.}

\texttt{[R8] IF ((odor=s)), THEN p.}

\texttt{[R9] IF ((odor=n) AND (spore-print-color=h)), THEN e.}

\texttt{[R10] IF ((odor=n) AND (spore-print-color=o)), THEN e.}

\texttt{[R11] IF ((odor=n) AND (spore-print-color=r)), THEN p.}

\texttt{[R12] IF ((odor=n) AND (spore-print-color=b)), THEN e.}

\texttt{[R13] IF ((odor=n) AND (spore-print-color=w) AND
  (stalk-root=c)), THEN p.}

\texttt{[R14] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=?)
  AND (gill-size=b)), THEN e.}

\texttt{[R15] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=?)
  AND (gill-size=n)), THEN p.}

\texttt{[R16] IF ((odor=n) AND (spore-print-color=w) AND
  (stalk-root=r)), THEN e.}

\texttt{[R17] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=z)), THEN
e.}

\texttt{[R18] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=c)), THEN e.}

\texttt{[R19] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=r)), THEN e.}

\texttt{[R20] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=b)), THEN e.}

\texttt{[R21] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=p)), THEN e.}

\texttt{[R22] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=w)), THEN p.}

\texttt{[R23] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=y)), THEN e.}

\texttt{[R24] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=n)), THEN e.}

\texttt{[R25] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=e)), THEN e.}

\texttt{[R26] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=u)), THEN e.}

\texttt{[R27] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=b) AND
(cap-color=g)), THEN e.}

\texttt{[R28] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=u)), THEN
e.}

\texttt{[R29] IF ((odor=n) AND (spore-print-color=w) AND (stalk-root=e)), THEN
e.}

\texttt{[R30] IF ((odor=n) AND (spore-print-color=y)), THEN e.}

\texttt{[R31] IF ((odor=n) AND (spore-print-color=n)), THEN e.}

\texttt{[R32] IF ((odor=n) AND (spore-print-color=u)), THEN e.}

\texttt{[R33] IF ((odor=n) AND (spore-print-color=k)), THEN e.}



\end{document}
