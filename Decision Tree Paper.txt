Decision Tree Paper
Jon David
Jarrett Decker
Background on DTs
Typically, a machine learning algorithm works by taking a known set of data and devising a function that generalizes it. Computers allow us to define multidimensional data sets and find these functions almost effortlessly. One of the drawbacks of these approaches, however, is that these functions are usually incomprehensible to humans. Most machine learning algorithms also only deal with numerical data. Decision Trees are a method which overcomes both of these shortcomings. Decision Trees create a system of rules that a human could read and understand, and when the rules are applied to new data, allows for fast classification.      
Our Problem
The problem we have been asked to solve is to classify mushrooms as either edible or poisonous. Our data contains 22 different non-numeric attributes to help us in this endeavor. This is a perfect candidate for the use of a Decision Tree and will allow us to hopefully come up with a few rules to help us identify poisonous mushrooms in the real world.  
Our Implementation
As per the project instructions our Decision Tree is modeled as the ID3 algorithm. In an effort to maintain good software engineering practices, we also chose to use object oriented programming methodologies when coding our Decision Tree. Our first challenge was getting the data in a structure that we could manage and manipulate. We then started defining our main classes for the tree. We implemented both entropy and misclassification error for our information gain metrics. After we had information gain we could then construct the tree. Pruning via chi square calculations was the next mechanic needed. We then were able to create our classification functionality.   
Data Structures
Data sets lie at the heart of all machine learning methods so being able to work with the data is very important. The approach we took was to create two classes, a database class, and a datadef class. The database class would essentially be a list of records created from the raw data set we were given. A record is a set of dictionaries where the dictionary key is a string representing an attribute and the value is the value of that attribute, for every attribute of a single row in our data set. The datadef class reads in a definition file that lists every attribute and every possible valid symbol for that attribute. This was useful for iterating through all possible symbols of an attribute. This approach would also allow us to possibly define new types of data sets at runtime, instead of having to hard code attribute definitions. 
We also wrote a few helper functions to grab a single column of a database. For instance, grabbing a vector of just the labels of each record in a database was an easy way to put together an output file after classification of that database. 
Utilities
{NOT SURE IF ANYTHING ACTUALLY WILL GO IN HERE}
       Tree Construction
Tree Infrastructure
Our tree is made up of a few classes. The ID3Tree class is the highest level class which is composed of ID3Nodes and ID3Edges. The ID3Node class is inherited from by the ID3DecisionNode and ID3LeafNode classes. Leaf nodes only keep track of the label they represent while decision nodes know what attribute they split on, how much information gain they represent, and their chi squared value. Edges have a source node and a destination node, the attribute that caused the split, and what value specifically this edge represents. This infrastructure allowed us to see exactly how the tree was architected and comprehend what was going on at any point in the tree. The ID3Tree class implemented most of the functions needed to construct the tree as well as interact with it through classification.  
Information Gain Metrics
The two methods of information gain we needed to use were entropy and misclassification error. We created two classes to deal with this, InformationGainCriteria and ClassificationErrorCriteria. The purpose of these classes were to be able to read through a database and then produce the best attribute candidate for splitting the database at the given node. Both of these methods work in similar ways. Entropy is calculated as -P(x)log(P(x)) and InformationGainCriteria would take the entropy at the parent node and subtract the weighted entropy of each of the child nodes after a simulated split for each possible attribute {Maybe include the formula instead of writing it out}. The attribute that caused the greatest value was then chosen to split on. Classifcation error is calculated by counting the instances of each of the possible labels for a database and then subtracting from 1 the probability of the highest occurring label {Again, show don’t tell?}. This is calculated for every attribute and the attribute resulting in the highest value is chosen to split on. These methods gave us a way our tree to decide how to construct itself. 
Pruning (Chi Square)
As with other machine learning algorithms, overfitting can be an issue with decision trees. To combat this we implemented a chi square check that would determine if our split was creating enough information to be useful. {I’m actually still not sure how the chi square stuff works so we’ll have to go through this part later}

ID3 Algorithm
With our infrastructure, information gain functions, and our chi square check, we could now create a decision tree with the ID3 algorithm. The algorithm works by taking in a database object, an information gain object, attribute definitions, and a chi square table with a confidence interval. A node is created for the database and then the database is checked for class impurities. If the database only has a single label then it is deemed pure and the node is returned as a leaf node. If the node isn’t pure then the database is analyzed by the information gain class to find a candidate attribute to split on. {stuff with chi square happens too} When an attribute is found the ID3 algorithm recursively calls itself to create nodes out of the split portions of the database and finishes either when each node hits a pure set or when the chi square test halts splitting. This function returns a tree object which can then be used for classification.
Classification
{Stuff about classification}
Results
{Talk about how the data set we were given worked out for us, it worked very well}
Explanation of Results (which options work well and why)
{I’m not sure if there’s really anything to say here since running it for any configuration yields basically the same results}
Folklore Rules
{I’m thinking odor is probably the most important thing to remember, possibly the only thing you should really care about? Would have to look at the tree closer to see how many things actually make it passed the odor check}
Conclusions? Maybe? 







